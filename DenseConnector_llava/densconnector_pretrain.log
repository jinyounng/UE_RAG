[2025-10-07 12:04:11,588] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-07 12:04:13,397] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-10-07 12:04:13,397] [INFO] [runner.py:570:main] cmd = /data3/jykim/anaconda3/envs/dc/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None dc/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path lmsys/vicuna-7b-v1.5 --version plain --data_path /data3/DB/dataset/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json --image_folder /data3/DB/dataset/LLaVA-Pretrain --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_dense_connector_type dci --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./checkpoints_stage1/DenseConnector-v1.5-7b-Pretrain --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 16 --evaluation_strategy no --save_strategy steps --save_steps 500 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb
[2025-10-07 12:04:15,076] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-07 12:04:16,839] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-10-07 12:04:16,839] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-10-07 12:04:16,839] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-10-07 12:04:16,839] [INFO] [launch.py:163:main] dist_world_size=2
[2025-10-07 12:04:16,839] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-10-07 12:04:19,223] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-07 12:04:19,278] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-07 12:04:21,608] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-07 12:04:21,799] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-10-07 12:04:21,799] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/data3/jykim/anaconda3/envs/dc/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/data3/jykim/anaconda3/envs/dc/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:05<00:05,  5.73s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:05<00:05,  5.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.50s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.83s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.90s/it]
/data3/jykim/anaconda3/envs/dc/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data3/jykim/anaconda3/envs/dc/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data3/jykim/anaconda3/envs/dc/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/data3/jykim/anaconda3/envs/dc/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
Formatting inputs...Skip in lazy mode
wandb: Currently logged in as: wlsdud338 (wlsdud338-chung-ang-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run f01jwcz7
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /data3/jykim/Projects/VLM/DenseConnector_llava/wandb/run-20251007_120447-f01jwcz7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-sun-13
wandb: â­ï¸ View project at https://wandb.ai/wlsdud338-chung-ang-university/huggingface
wandb: ðŸš€ View run at https://wandb.ai/wlsdud338-chung-ang-university/huggingface/runs/f01jwcz7
  0%|          | 0/2180 [00:00<?, ?it/s]/data3/jykim/anaconda3/envs/dc/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/data3/jykim/anaconda3/envs/dc/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
  0%|          | 1/2180 [00:28<17:10:54, 28.39s/it]                                                   {'loss': 7.797, 'learning_rate': 1.5151515151515153e-05, 'epoch': 0.0}
  0%|          | 1/2180 [00:28<17:10:54, 28.39s/it]  0%|          | 2/2180 [00:55<16:46:15, 27.72s/it]                                                   {'loss': 7.7449, 'learning_rate': 3.0303030303030306e-05, 'epoch': 0.0}
  0%|          | 2/2180 [00:55<16:46:15, 27.72s/it]  0%|          | 3/2180 [01:23<16:49:31, 27.82s/it]                                                   {'loss': 7.2456, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.0}
  0%|          | 3/2180 [01:23<16:49:31, 27.82s/it]  0%|          | 4/2180 [01:51<16:55:16, 27.99s/it]                                                   {'loss': 5.8795, 'learning_rate': 6.060606060606061e-05, 'epoch': 0.0}
  0%|          | 4/2180 [01:51<16:55:16, 27.99s/it]  0%|          | 5/2180 [02:20<17:00:21, 28.15s/it]                                                   {'loss': 5.7082, 'learning_rate': 7.575757575757576e-05, 'epoch': 0.0}
  0%|          | 5/2180 [02:20<17:00:21, 28.15s/it]  0%|          | 6/2180 [02:48<17:03:53, 28.26s/it]                                                   {'loss': 5.4523, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.0}
  0%|          | 6/2180 [02:48<17:03:53, 28.26s/it]  0%|          | 7/2180 [03:17<17:07:11, 28.36s/it]                                                   {'loss': 5.2531, 'learning_rate': 0.00010606060606060606, 'epoch': 0.0}
  0%|          | 7/2180 [03:17<17:07:11, 28.36s/it]  0%|          | 8/2180 [03:46<17:11:01, 28.48s/it]                                                   {'loss': 5.0159, 'learning_rate': 0.00012121212121212122, 'epoch': 0.0}
  0%|          | 8/2180 [03:46<17:11:01, 28.48s/it]  0%|          | 9/2180 [04:14<17:11:22, 28.50s/it]                                                   {'loss': 4.9588, 'learning_rate': 0.00013636363636363637, 'epoch': 0.0}
  0%|          | 9/2180 [04:14<17:11:22, 28.50s/it]  0%|          | 10/2180 [04:43<17:10:23, 28.49s/it]                                                    {'loss': 4.7741, 'learning_rate': 0.00015151515151515152, 'epoch': 0.0}
  0%|          | 10/2180 [04:43<17:10:23, 28.49s/it]  1%|          | 11/2180 [05:11<17:11:59, 28.55s/it]                                                    {'loss': 4.5394, 'learning_rate': 0.00016666666666666666, 'epoch': 0.01}
  1%|          | 11/2180 [05:11<17:11:59, 28.55s/it]  1%|          | 12/2180 [05:40<17:13:24, 28.60s/it]                                                    {'loss': 4.3426, 'learning_rate': 0.00018181818181818183, 'epoch': 0.01}
  1%|          | 12/2180 [05:40<17:13:24, 28.60s/it]  1%|          | 13/2180 [06:09<17:14:02, 28.63s/it]                                                    {'loss': 4.2729, 'learning_rate': 0.00019696969696969695, 'epoch': 0.01}
  1%|          | 13/2180 [06:09<17:14:02, 28.63s/it]  1%|          | 14/2180 [06:37<17:15:15, 28.68s/it]                                                    {'loss': 4.122, 'learning_rate': 0.00021212121212121213, 'epoch': 0.01}
  1%|          | 14/2180 [06:37<17:15:15, 28.68s/it]  1%|          | 15/2180 [07:06<17:15:14, 28.69s/it]                                                    {'loss': 4.0053, 'learning_rate': 0.00022727272727272727, 'epoch': 0.01}
  1%|          | 15/2180 [07:06<17:15:14, 28.69s/it]  1%|          | 16/2180 [07:35<17:14:06, 28.67s/it]                                                    {'loss': 3.8213, 'learning_rate': 0.00024242424242424245, 'epoch': 0.01}
  1%|          | 16/2180 [07:35<17:14:06, 28.67s/it]  1%|          | 17/2180 [08:03<17:12:31, 28.64s/it]                                                    {'loss': 3.8801, 'learning_rate': 0.00025757575757575756, 'epoch': 0.01}
  1%|          | 17/2180 [08:03<17:12:31, 28.64s/it]  1%|          | 18/2180 [08:32<17:12:05, 28.64s/it]                                                    {'loss': 3.7525, 'learning_rate': 0.00027272727272727274, 'epoch': 0.01}
  1%|          | 18/2180 [08:32<17:12:05, 28.64s/it]  1%|          | 19/2180 [09:01<17:10:48, 28.62s/it]                                                    {'loss': 3.6866, 'learning_rate': 0.0002878787878787879, 'epoch': 0.01}
  1%|          | 19/2180 [09:01<17:10:48, 28.62s/it]  1%|          | 20/2180 [09:29<17:10:36, 28.63s/it]                                                    {'loss': 3.547, 'learning_rate': 0.00030303030303030303, 'epoch': 0.01}
  1%|          | 20/2180 [09:29<17:10:36, 28.63s/it]  1%|          | 21/2180 [09:58<17:10:42, 28.64s/it]                                                    {'loss': 3.5408, 'learning_rate': 0.0003181818181818182, 'epoch': 0.01}
  1%|          | 21/2180 [09:58<17:10:42, 28.64s/it]  1%|          | 22/2180 [10:27<17:12:43, 28.71s/it]                                                    {'loss': 3.4557, 'learning_rate': 0.0003333333333333333, 'epoch': 0.01}
  1%|          | 22/2180 [10:27<17:12:43, 28.71s/it]  1%|          | 23/2180 [10:55<17:11:14, 28.69s/it]                                                    {'loss': 3.3629, 'learning_rate': 0.0003484848484848485, 'epoch': 0.01}
  1%|          | 23/2180 [10:55<17:11:14, 28.69s/it]  1%|          | 24/2180 [11:24<17:10:54, 28.69s/it]                                                    {'loss': 3.5297, 'learning_rate': 0.00036363636363636367, 'epoch': 0.01}
  1%|          | 24/2180 [11:24<17:10:54, 28.69s/it]  1%|          | 25/2180 [11:53<17:08:50, 28.65s/it]                                                    {'loss': 3.4251, 'learning_rate': 0.0003787878787878788, 'epoch': 0.01}
  1%|          | 25/2180 [11:53<17:08:50, 28.65s/it]  1%|          | 26/2180 [12:21<17:05:31, 28.57s/it]                                                    {'loss': 3.2874, 'learning_rate': 0.0003939393939393939, 'epoch': 0.01}
  1%|          | 26/2180 [12:21<17:05:31, 28.57s/it]  1%|          | 27/2180 [12:50<17:06:48, 28.62s/it]                                                    {'loss': 3.4641, 'learning_rate': 0.00040909090909090913, 'epoch': 0.01}
  1%|          | 27/2180 [12:50<17:06:48, 28.62s/it]  1%|â–         | 28/2180 [13:18<17:06:46, 28.63s/it]                                                    {'loss': 3.4669, 'learning_rate': 0.00042424242424242425, 'epoch': 0.01}
  1%|â–         | 28/2180 [13:18<17:06:46, 28.63s/it]  1%|â–         | 29/2180 [13:47<17:06:34, 28.64s/it]                                                    {'loss': 3.3252, 'learning_rate': 0.0004393939393939394, 'epoch': 0.01}
  1%|â–         | 29/2180 [13:47<17:06:34, 28.64s/it]  1%|â–         | 30/2180 [14:16<17:05:57, 28.63s/it]                                                    {'loss': 3.4405, 'learning_rate': 0.00045454545454545455, 'epoch': 0.01}
  1%|â–         | 30/2180 [14:16<17:05:57, 28.63s/it]  1%|â–         | 31/2180 [14:44<17:05:49, 28.64s/it]                                                    {'loss': 3.3604, 'learning_rate': 0.0004696969696969697, 'epoch': 0.01}
  1%|â–         | 31/2180 [14:44<17:05:49, 28.64s/it]  1%|â–         | 32/2180 [15:13<17:04:56, 28.63s/it]                                                    {'loss': 3.326, 'learning_rate': 0.0004848484848484849, 'epoch': 0.01}
  1%|â–         | 32/2180 [15:13<17:04:56, 28.63s/it]  2%|â–         | 33/2180 [15:42<17:04:37, 28.63s/it]                                                    {'loss': 3.292, 'learning_rate': 0.0005, 'epoch': 0.02}
  2%|â–         | 33/2180 [15:42<17:04:37, 28.63s/it]  2%|â–         | 34/2180 [16:10<17:03:33, 28.62s/it]                                                    {'loss': 3.3669, 'learning_rate': 0.0005151515151515151, 'epoch': 0.02}
  2%|â–         | 34/2180 [16:10<17:03:33, 28.62s/it]  2%|â–         | 35/2180 [16:39<17:02:13, 28.59s/it]                                                    {'loss': 3.301, 'learning_rate': 0.0005303030303030302, 'epoch': 0.02}
  2%|â–         | 35/2180 [16:39<17:02:13, 28.59s/it]  2%|â–         | 36/2180 [17:07<17:00:57, 28.57s/it]                                                    {'loss': 3.2788, 'learning_rate': 0.0005454545454545455, 'epoch': 0.02}
  2%|â–         | 36/2180 [17:07<17:00:57, 28.57s/it]  2%|â–         | 37/2180 [17:36<17:00:06, 28.56s/it]                                                    {'loss': 3.3627, 'learning_rate': 0.0005606060606060606, 'epoch': 0.02}
  2%|â–         | 37/2180 [17:36<17:00:06, 28.56s/it]  2%|â–         | 38/2180 [18:04<16:58:07, 28.52s/it]                                                    {'loss': 3.3783, 'learning_rate': 0.0005757575757575758, 'epoch': 0.02}
  2%|â–         | 38/2180 [18:04<16:58:07, 28.52s/it]  2%|â–         | 39/2180 [18:33<16:57:25, 28.51s/it]                                                    {'loss': 3.4418, 'learning_rate': 0.0005909090909090909, 'epoch': 0.02}
  2%|â–         | 39/2180 [18:33<16:57:25, 28.51s/it]  2%|â–         | 40/2180 [19:01<16:57:06, 28.52s/it]                                                    {'loss': 3.3781, 'learning_rate': 0.0006060606060606061, 'epoch': 0.02}
  2%|â–         | 40/2180 [19:01<16:57:06, 28.52s/it]  2%|â–         | 41/2180 [19:30<16:59:36, 28.60s/it]                                                    {'loss': 3.2324, 'learning_rate': 0.0006212121212121212, 'epoch': 0.02}
  2%|â–         | 41/2180 [19:30<16:59:36, 28.60s/it]  2%|â–         | 42/2180 [19:59<16:59:22, 28.61s/it]                                                    {'loss': 3.3404, 'learning_rate': 0.0006363636363636364, 'epoch': 0.02}
  2%|â–         | 42/2180 [19:59<16:59:22, 28.61s/it]  2%|â–         | 43/2180 [20:27<16:58:22, 28.59s/it]                                                    {'loss': 3.3119, 'learning_rate': 0.0006515151515151515, 'epoch': 0.02}
  2%|â–         | 43/2180 [20:27<16:58:22, 28.59s/it]  2%|â–         | 44/2180 [20:56<16:57:58, 28.59s/it]                                                    {'loss': 3.2254, 'learning_rate': 0.0006666666666666666, 'epoch': 0.02}
  2%|â–         | 44/2180 [20:56<16:57:58, 28.59s/it]  2%|â–         | 45/2180 [21:24<16:58:01, 28.61s/it]                                                    {'loss': 3.1584, 'learning_rate': 0.0006818181818181818, 'epoch': 0.02}
  2%|â–         | 45/2180 [21:24<16:58:01, 28.61s/it]  2%|â–         | 46/2180 [21:53<16:58:14, 28.63s/it]                                                    {'loss': 3.2704, 'learning_rate': 0.000696969696969697, 'epoch': 0.02}
  2%|â–         | 46/2180 [21:53<16:58:14, 28.63s/it]  2%|â–         | 47/2180 [22:22<16:57:01, 28.61s/it]                                                    {'loss': 3.2766, 'learning_rate': 0.0007121212121212122, 'epoch': 0.02}
  2%|â–         | 47/2180 [22:22<16:57:01, 28.61s/it]  2%|â–         | 48/2180 [22:50<16:57:10, 28.63s/it]                                                    {'loss': 3.2736, 'learning_rate': 0.0007272727272727273, 'epoch': 0.02}
  2%|â–         | 48/2180 [22:50<16:57:10, 28.63s/it]  2%|â–         | 49/2180 [23:19<16:57:26, 28.65s/it]                                                    {'loss': 3.2642, 'learning_rate': 0.0007424242424242425, 'epoch': 0.02}
  2%|â–         | 49/2180 [23:19<16:57:26, 28.65s/it]  2%|â–         | 50/2180 [23:48<16:56:41, 28.64s/it]                                                    {'loss': 3.3391, 'learning_rate': 0.0007575757575757576, 'epoch': 0.02}
  2%|â–         | 50/2180 [23:48<16:56:41, 28.64s/it]  2%|â–         | 51/2180 [24:16<16:56:02, 28.63s/it]                                                    {'loss': 3.2425, 'learning_rate': 0.0007727272727272727, 'epoch': 0.02}
  2%|â–         | 51/2180 [24:16<16:56:02, 28.63s/it]  2%|â–         | 52/2180 [24:45<16:54:39, 28.61s/it]                                                    {'loss': 3.2937, 'learning_rate': 0.0007878787878787878, 'epoch': 0.02}
  2%|â–         | 52/2180 [24:45<16:54:39, 28.61s/it]  2%|â–         | 53/2180 [25:13<16:54:49, 28.63s/it]                                                    {'loss': 3.2084, 'learning_rate': 0.000803030303030303, 'epoch': 0.02}
  2%|â–         | 53/2180 [25:14<16:54:49, 28.63s/it]  2%|â–         | 54/2180 [25:42<16:54:04, 28.62s/it]                                                    {'loss': 3.2732, 'learning_rate': 0.0008181818181818183, 'epoch': 0.02}
  2%|â–         | 54/2180 [25:42<16:54:04, 28.62s/it]  3%|â–Ž         | 55/2180 [26:11<16:52:45, 28.60s/it]                                                    {'loss': 3.2897, 'learning_rate': 0.0008333333333333334, 'epoch': 0.03}
  3%|â–Ž         | 55/2180 [26:11<16:52:45, 28.60s/it]  3%|â–Ž         | 56/2180 [26:39<16:51:42, 28.58s/it]                                                    {'loss': 3.337, 'learning_rate': 0.0008484848484848485, 'epoch': 0.03}
  3%|â–Ž         | 56/2180 [26:39<16:51:42, 28.58s/it]  3%|â–Ž         | 57/2180 [27:08<16:52:42, 28.62s/it]                                                    {'loss': 3.1911, 'learning_rate': 0.0008636363636363636, 'epoch': 0.03}
  3%|â–Ž         | 57/2180 [27:08<16:52:42, 28.62s/it]  3%|â–Ž         | 58/2180 [27:36<16:51:24, 28.60s/it]                                                    {'loss': 3.2436, 'learning_rate': 0.0008787878787878789, 'epoch': 0.03}
  3%|â–Ž         | 58/2180 [27:36<16:51:24, 28.60s/it]  3%|â–Ž         | 59/2180 [28:05<16:50:50, 28.60s/it]                                                    {'loss': 3.195, 'learning_rate': 0.000893939393939394, 'epoch': 0.03}
  3%|â–Ž         | 59/2180 [28:05<16:50:50, 28.60s/it]  3%|â–Ž         | 60/2180 [28:34<16:51:37, 28.63s/it]                                                    {'loss': 3.2177, 'learning_rate': 0.0009090909090909091, 'epoch': 0.03}
  3%|â–Ž         | 60/2180 [28:34<16:51:37, 28.63s/it]  3%|â–Ž         | 61/2180 [29:02<16:49:17, 28.58s/it]                                                    {'loss': 3.1635, 'learning_rate': 0.0009242424242424242, 'epoch': 0.03}
  3%|â–Ž         | 61/2180 [29:02<16:49:17, 28.58s/it]  3%|â–Ž         | 62/2180 [29:31<16:49:55, 28.61s/it]                                                    {'loss': 3.1393, 'learning_rate': 0.0009393939393939394, 'epoch': 0.03}
  3%|â–Ž         | 62/2180 [29:31<16:49:55, 28.61s/it]  3%|â–Ž         | 63/2180 [30:00<16:49:49, 28.62s/it]                                                    {'loss': 3.1629, 'learning_rate': 0.0009545454545454546, 'epoch': 0.03}
  3%|â–Ž         | 63/2180 [30:00<16:49:49, 28.62s/it]  3%|â–Ž         | 64/2180 [30:28<16:50:18, 28.65s/it]                                                    {'loss': 3.0884, 'learning_rate': 0.0009696969696969698, 'epoch': 0.03}
  3%|â–Ž         | 64/2180 [30:28<16:50:18, 28.65s/it]  3%|â–Ž         | 65/2180 [30:57<16:50:22, 28.66s/it]                                                    {'loss': 3.1177, 'learning_rate': 0.000984848484848485, 'epoch': 0.03}
  3%|â–Ž         | 65/2180 [30:57<16:50:22, 28.66s/it]  3%|â–Ž         | 66/2180 [31:26<16:50:04, 28.67s/it]                                                    {'loss': 3.0242, 'learning_rate': 0.001, 'epoch': 0.03}
  3%|â–Ž         | 66/2180 [31:26<16:50:04, 28.67s/it]  3%|â–Ž         | 67/2180 [31:54<16:50:07, 28.68s/it]                                                    {'loss': 3.0273, 'learning_rate': 0.0009999994478847943, 'epoch': 0.03}
  3%|â–Ž         | 67/2180 [31:54<16:50:07, 28.68s/it]  3%|â–Ž         | 68/2180 [32:23<16:49:15, 28.67s/it]                                                    {'loss': 3.0259, 'learning_rate': 0.0009999977915403962, 'epoch': 0.03}
  3%|â–Ž         | 68/2180 [32:23<16:49:15, 28.67s/it]  3%|â–Ž         | 69/2180 [32:52<16:49:26, 28.69s/it]                                                    {'loss': 2.9741, 'learning_rate': 0.0009999950309704639, 'epoch': 0.03}
  3%|â–Ž         | 69/2180 [32:52<16:49:26, 28.69s/it]  3%|â–Ž         | 70/2180 [33:20<16:49:01, 28.69s/it]                                                    {'loss': 3.0409, 'learning_rate': 0.000999991166181094, 'epoch': 0.03}
  3%|â–Ž         | 70/2180 [33:20<16:49:01, 28.69s/it]  3%|â–Ž         | 71/2180 [33:49<16:48:25, 28.69s/it]                                                    {'loss': 2.947, 'learning_rate': 0.0009999861971808216, 'epoch': 0.03}
  3%|â–Ž         | 71/2180 [33:49<16:48:25, 28.69s/it]  3%|â–Ž         | 72/2180 [34:18<16:48:28, 28.70s/it]                                                    {'loss': 2.9726, 'learning_rate': 0.0009999801239806208, 'epoch': 0.03}
  3%|â–Ž         | 72/2180 [34:18<16:48:28, 28.70s/it]  3%|â–Ž         | 73/2180 [34:47<16:48:03, 28.71s/it]                                                    {'loss': 2.8339, 'learning_rate': 0.0009999729465939035, 'epoch': 0.03}
  3%|â–Ž         | 73/2180 [34:47<16:48:03, 28.71s/it]  3%|â–Ž         | 74/2180 [35:15<16:47:07, 28.69s/it]                                                    {'loss': 2.9729, 'learning_rate': 0.0009999646650365212, 'epoch': 0.03}
  3%|â–Ž         | 74/2180 [35:15<16:47:07, 28.69s/it]  3%|â–Ž         | 75/2180 [35:44<16:46:37, 28.69s/it]                                                    {'loss': 2.8747, 'learning_rate': 0.0009999552793267634, 'epoch': 0.03}
  3%|â–Ž         | 75/2180 [35:44<16:46:37, 28.69s/it]  3%|â–Ž         | 76/2180 [36:13<16:47:02, 28.72s/it]                                                    {'loss': 2.8916, 'learning_rate': 0.0009999447894853577, 'epoch': 0.03}
  3%|â–Ž         | 76/2180 [36:13<16:47:02, 28.72s/it]  4%|â–Ž         | 77/2180 [36:41<16:45:05, 28.68s/it]                                                    {'loss': 2.9345, 'learning_rate': 0.0009999331955354708, 'epoch': 0.04}
  4%|â–Ž         | 77/2180 [36:41<16:45:05, 28.68s/it]  4%|â–Ž         | 78/2180 [37:10<16:44:51, 28.68s/it]                                                    {'loss': 2.7488, 'learning_rate': 0.0009999204975027073, 'epoch': 0.04}
  4%|â–Ž         | 78/2180 [37:10<16:44:51, 28.68s/it]  4%|â–Ž         | 79/2180 [37:39<16:44:36, 28.69s/it]                                                    {'loss': 2.7539, 'learning_rate': 0.0009999066954151103, 'epoch': 0.04}
  4%|â–Ž         | 79/2180 [37:39<16:44:36, 28.69s/it]  4%|â–Ž         | 80/2180 [38:07<16:43:14, 28.66s/it]                                                    {'loss': 2.8837, 'learning_rate': 0.0009998917893031614, 'epoch': 0.04}
  4%|â–Ž         | 80/2180 [38:07<16:43:14, 28.66s/it]  4%|â–Ž         | 81/2180 [38:36<16:42:38, 28.66s/it]                                                    {'loss': 2.9043, 'learning_rate': 0.0009998757791997801, 'epoch': 0.04}
  4%|â–Ž         | 81/2180 [38:36<16:42:38, 28.66s/it]  4%|â–         | 82/2180 [39:05<16:43:21, 28.69s/it]                                                    {'loss': 2.8381, 'learning_rate': 0.0009998586651403238, 'epoch': 0.04}
  4%|â–         | 82/2180 [39:05<16:43:21, 28.69s/it]  4%|â–         | 83/2180 [39:33<16:42:40, 28.69s/it]                                                    {'loss': 2.7366, 'learning_rate': 0.0009998404471625885, 'epoch': 0.04}
  4%|â–         | 83/2180 [39:33<16:42:40, 28.69s/it]  4%|â–         | 84/2180 [40:02<16:43:52, 28.74s/it]                                                    {'loss': 2.6679, 'learning_rate': 0.0009998211253068078, 'epoch': 0.04}
  4%|â–         | 84/2180 [40:02<16:43:52, 28.74s/it]  4%|â–         | 85/2180 [40:31<16:42:43, 28.72s/it]                                                    {'loss': 2.8218, 'learning_rate': 0.0009998006996156535, 'epoch': 0.04}
  4%|â–         | 85/2180 [40:31<16:42:43, 28.72s/it]  4%|â–         | 86/2180 [41:00<16:42:52, 28.74s/it]                                                    {'loss': 2.6444, 'learning_rate': 0.0009997791701342347, 'epoch': 0.04}
  4%|â–         | 86/2180 [41:00<16:42:52, 28.74s/it]  4%|â–         | 87/2180 [41:28<16:39:56, 28.67s/it]                                                    {'loss': 2.7138, 'learning_rate': 0.0009997565369100983, 'epoch': 0.04}
  4%|â–         | 87/2180 [41:28<16:39:56, 28.67s/it]  4%|â–         | 88/2180 [41:57<16:39:20, 28.66s/it]                                                    {'loss': 2.6365, 'learning_rate': 0.0009997327999932291, 'epoch': 0.04}
  4%|â–         | 88/2180 [41:57<16:39:20, 28.66s/it]  4%|â–         | 89/2180 [42:25<16:38:42, 28.66s/it]                                                    {'loss': 2.6696, 'learning_rate': 0.000999707959436049, 'epoch': 0.04}
  4%|â–         | 89/2180 [42:25<16:38:42, 28.66s/it]  4%|â–         | 90/2180 [42:54<16:38:14, 28.66s/it]                                                    {'loss': 2.681, 'learning_rate': 0.0009996820152934176, 'epoch': 0.04}
  4%|â–         | 90/2180 [42:54<16:38:14, 28.66s/it]  4%|â–         | 91/2180 [43:23<16:38:51, 28.69s/it]                                                    {'loss': 2.645, 'learning_rate': 0.000999654967622631, 'epoch': 0.04}
  4%|â–         | 91/2180 [43:23<16:38:51, 28.69s/it]  4%|â–         | 92/2180 [43:52<16:38:44, 28.70s/it]                                                    {'loss': 2.6386, 'learning_rate': 0.0009996268164834238, 'epoch': 0.04}
  4%|â–         | 92/2180 [43:52<16:38:44, 28.70s/it]  4%|â–         | 93/2180 [44:20<16:38:53, 28.72s/it]                                                    {'loss': 2.5958, 'learning_rate': 0.000999597561937966, 'epoch': 0.04}
  4%|â–         | 93/2180 [44:20<16:38:53, 28.72s/it]  4%|â–         | 94/2180 [44:49<16:40:31, 28.78s/it]                                                    {'loss': 2.5729, 'learning_rate': 0.0009995672040508656, 'epoch': 0.04}
  4%|â–         | 94/2180 [44:49<16:40:31, 28.78s/it]  4%|â–         | 95/2180 [45:18<16:40:20, 28.79s/it]                                                    {'loss': 2.5664, 'learning_rate': 0.0009995357428891662, 'epoch': 0.04}
  4%|â–         | 95/2180 [45:18<16:40:20, 28.79s/it]  4%|â–         | 96/2180 [45:47<16:39:45, 28.78s/it]                                                    {'loss': 2.595, 'learning_rate': 0.0009995031785223491, 'epoch': 0.04}
  4%|â–         | 96/2180 [45:47<16:39:45, 28.78s/it]  4%|â–         | 97/2180 [46:16<16:40:53, 28.83s/it]                                                    {'loss': 2.692, 'learning_rate': 0.000999469511022331, 'epoch': 0.04}
  4%|â–         | 97/2180 [46:16<16:40:53, 28.83s/it]  4%|â–         | 98/2180 [46:44<16:38:50, 28.79s/it]                                                    {'loss': 2.5003, 'learning_rate': 0.0009994347404634657, 'epoch': 0.04}
  4%|â–         | 98/2180 [46:44<16:38:50, 28.79s/it]  5%|â–         | 99/2180 [47:13<16:36:44, 28.74s/it]                                                    {'loss': 2.6292, 'learning_rate': 0.0009993988669225423, 'epoch': 0.05}
  5%|â–         | 99/2180 [47:13<16:36:44, 28.74s/it]  5%|â–         | 100/2180 [47:42<16:34:19, 28.68s/it]                                                     {'loss': 2.5438, 'learning_rate': 0.000999361890478786, 'epoch': 0.05}
  5%|â–         | 100/2180 [47:42<16:34:19, 28.68s/it]  5%|â–         | 101/2180 [48:10<16:33:58, 28.69s/it]                                                     {'loss': 2.6228, 'learning_rate': 0.0009993238112138583, 'epoch': 0.05}
  5%|â–         | 101/2180 [48:10<16:33:58, 28.69s/it]  5%|â–         | 102/2180 [48:39<16:33:47, 28.69s/it]                                                     {'loss': 2.4836, 'learning_rate': 0.0009992846292118554, 'epoch': 0.05}
  5%|â–         | 102/2180 [48:39<16:33:47, 28.69s/it]  5%|â–         | 103/2180 [49:08<16:35:01, 28.74s/it]                                                     {'loss': 2.5276, 'learning_rate': 0.000999244344559309, 'epoch': 0.05}
  5%|â–         | 103/2180 [49:08<16:35:01, 28.74s/it]  5%|â–         | 104/2180 [49:37<16:34:40, 28.75s/it]                                                     {'loss': 2.4243, 'learning_rate': 0.0009992029573451869, 'epoch': 0.05}
  5%|â–         | 104/2180 [49:37<16:34:40, 28.75s/it]  5%|â–         | 105/2180 [50:05<16:33:07, 28.72s/it]                                                     {'loss': 2.4268, 'learning_rate': 0.0009991604676608905, 'epoch': 0.05}
  5%|â–         | 105/2180 [50:05<16:33:07, 28.72s/it]  5%|â–         | 106/2180 [50:34<16:31:32, 28.68s/it]                                                     {'loss': 2.4703, 'learning_rate': 0.0009991168756002568, 'epoch': 0.05}
  5%|â–         | 106/2180 [50:34<16:31:32, 28.68s/it]